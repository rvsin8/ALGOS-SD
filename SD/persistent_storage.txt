# Persistent Storage

This is one topic that needs little introduction in today’s day and age. Today, where vast amounts of data are generated, collected, and processed every second, databases and persistent storage systems play a pivotal role in storing, organizing, managing, and retrieving information efficiently. From e-commerce platforms and social media networks to banking systems and healthcare applications, databases serve as the backbone of data management, ensuring data integrity, availability, and accessibility.

Databases serve as centralized repositories for storing data. Data could be structured, semi-structured or unstructured data, and database systems allow applications to access and manipulate information efficiently. By providing a structured framework for data storage and retrieval, databases enable organizations to organize, analyze, and derive insights from their data assets.

**Evolution over Time**

In today's world, where data is generated at an unprecedented rate, databases play a crucial role in enabling businesses to harness the power of data-driven decision-making. Whether it's tracking customer transactions, analyzing user behavior, or managing inventory levels, databases provide the foundation for businesses to optimize operations, drive innovation, and deliver personalized experiences to users. However, if we rewind back a few decades and analyze the evolution of databases over time, we’ll be amazed. Not only to see how technology has advanced but also how the exponential growth of data and changing business requirements have contributed to the evolution of storage systems over time. Here are some of the ways databases have evolved:

1. **Basic Data Storage to Advanced Query Capabilities:** Initially, databases focused on basic data storage and limited query patterns. With technology advancing, query capabilities expanded, allowing for more complex data analysis and reporting.
2. **Diverse Data Types and Use Cases:** As data volumes and variety increased, databases evolved to support a broader range of data types and specialized use cases. This led to the emergence of specialized database systems such as document databases, key-value stores, column-family stores, and graph databases, catering to specific data management requirements and use cases.
3. **Scalability Challenges and Cloud Adoption:** The rise of big data and cloud computing posed scalability challenges for traditional databases, leading to the adoption of cloud-native database solutions. Cloud databases offer scalability, flexibility, and cost-effectiveness for managing data at scale.
4. **Integration with Emerging Technologies:** Database systems are increasingly integrating with emerging technologies such as AI, ML, blockchain, and IoT to enhance data analytics, automation, and decision-making capabilities, enabling organizations to leverage data-driven insights and innovations.

**Online Databases and Offline Databases**

**Online Databases**

Online databases are designed to be continuously available and accessible for query and manipulation in real-time. They serve as the backend storage systems for various online applications, enabling users to interact with data dynamically.

**Examples are MySQL, PostgresSQL, MongoDB**. Here are some key characteristics and considerations of online databases:

1. **Real-Time Access**: Online databases support real-time access to data, allowing users and applications to retrieve, update, and manipulate data instantly. This real-time access is essential for applications that require immediate responses to user queries and interactions, such as e-commerce platforms, social media networks, and banking systems.
    1. Real-time access ensures data is **instantly available for reading or writing**. This is essential for applications where users expect immediate feedback.
    2. **Example:**
        - Imagine an **e-commerce site** where users add items to a cart. Real-time access ensures the inventory updates as soon as someone places an order.
        - Without real-time access, users might see outdated inventory, leading to overbooking or disappointment.
    3. Databases achieve this through **caching** and **connection pooling** to keep query response times low.
        - **How it works:** If a user queries the price of a product, the database serves it directly from cache instead of re-processing the query every time.
        - **Locking:** Only one user can modify a record at a time, e.g., editing a movie title locks the title field.
        - **Transactions:** Ensure all operations in a process succeed together. Example: Transferring money involves two steps: deducting from one account and adding to another. If one step fails, both are rolled back.
2. **Concurrent Access**: Online databases are designed to handle concurrent access from multiple users or applications simultaneously. They employ concurrency control mechanisms, such as locking, transactions, and isolation levels, to ensure data integrity and consistency in multi-user environments.
    1. Multiple users or applications can interact with the database at the same time without conflicts.
    2. **Example:**
        - Think of **Netflix**, where millions of users stream videos and add shows to watchlists simultaneously.
        - If two users try to modify the same show entry (e.g., add reviews), the database ensures the changes don’t corrupt each other.
3. **Optimized for Performance**: Online databases are optimized for performance to deliver low-latency responses to user queries and transactions. They employ indexing, query optimization, caching, and other performance-enhancing techniques to minimize response times and improve overall system efficiency.
    1. Online databases prioritize speed for both reading and writing operations, essential for seamless user experiences.
    2. **Example:**
        - **Instagram** posts appear instantly after you share them. The database must handle both the upload (write) and visibility (read) efficiently.
        - **Indexing:** Posts are stored in a way that finding "posts with #travel" is super-fast—like a search index in a book.
            - **Indexing:**
                - Speeds up searches, e.g., searching for all posts by a user. Without it, the database scans every post.
    3. **Query Optimization:**
        - A poorly written query (e.g., `SELECT * FROM posts`) fetches everything and slows the database. Query optimizers rewrite such queries for efficiency.
    4. **Replication:**
        - Duplicates data across servers so reads can be handled by replicas, while the main server handles writes.
        

**Question:** Why would a banking app fail without real-time access to data like account balances?

**Answer:** Without real-time access, users might see outdated balances, leading to overdrafts or fraudulent transactions as the system wouldn't reflect recent changes instantly.

**Question:** If two users try to book the last Airbnb at the same time, what prevents overbooking?

**Answer: Transactions** and **locking** ensure that only one user’s booking request is processed at a time, preventing the same property from being reserved twice.

**Question:** Why might indexing slow down write-heavy applications like real-time messaging?

**Answer:** Indexing requires updates every time data is inserted or modified, adding overhead to write operations, which can slow down systems with frequent writes like messaging apps.

**Offline Databases**

Offline databases, also known as batch databases, are used for storing and processing data in bulk at scheduled intervals, typically during periods of low activity or downtime. While offline databases are not accessible for real-time interactions, they play a crucial role in performing batch processing, data analysis, and reporting tasks.

**Examples are Apache Hadoop** (with its component called Apache Hive)

1. **Batch Processing:** Offline databases are used for batch processing tasks where data is processed in bulk at scheduled intervals or during predefined batch windows. This may involve tasks such as data extraction, transformation, loading (ETL), analytics, and reporting.
    1. Offline databases are ideal for tasks where data processing doesn’t need to happen instantly. For instance, calculating payroll, consolidating sales data, or updating customer records can be queued and executed in one go during off-hours. This allows systems to efficiently handle large datasets without impacting online services.
2. **Scheduled Updates:** Data in offline databases is updated periodically based on predefined schedules or triggers. These updates may involve data imports, exports, transformations, and aggregations performed offline without real-time user interactions.
    1. Offline databases update data periodically instead of in real-time. This is useful when immediate data consistency isn’t critical. For example, a retail chain might synchronize inventory levels from individual stores to a central system every night, avoiding constant synchronization overhead.
3. **Reduced Operational Overhead:** Since offline databases do not require continuous availability and real-time responsiveness, they typically have lower operational overhead compared to online databases. This allows organizations to optimize resource utilization and reduce infrastructure costs for offline processing tasks.
    1. Since offline databases don’t need to be always accessible, they require less expensive infrastructure. There’s no need for high-availability setups or optimized query performance during downtime. This translates into lower costs for storage and processing.
4. **Resource Efficiency:** Offline databases can leverage batch processing techniques to optimize resource utilization and achieve better performance efficiency for data-intensive tasks. They can process large volumes of data more efficiently by batching similar operations and maximizing parallelism.
    1. Offline databases excel at resource utilization by handling large datasets in bulk. By grouping similar operations—like aggregations or data cleaning—they minimize repeated tasks and make better use of available computing power, often leveraging parallel processing.
5. **Data Analysis and Reporting:** Offline databases are often used for data analysis, reporting, and business intelligence (BI) purposes, where historical data is analyzed to derive insights, trends, and patterns over time. They enable organizations to perform complex analytics and generate reports based on aggregated data from multiple sources.
    1. Offline databases are perfect for tasks like business intelligence and historical trend analysis. These computations can be complex and time-consuming but don’t need to run in real time. An example would be generating quarterly sales reports or identifying trends from years of customer data.

**Question: It seems like online databases give us the flexibility of real time updates and queries. Why use offline databases when you can just put in all your data in online databases?**

The short answer is that *there is no free lunch*.

1. **Cost**. Since offline databases are in some sense “read only” (don't allow real time updates) and not real time, they can be a lot more cost effective than real time database systems. The reason being that real time database systems consume a lot more hardware and software resources with the real time features they provide.
2. **Query Patterns and performance** Some data processing tasks are inherently batch-oriented and do not require real-time processing. For example, large-scale data analysis, reporting, and data transformation tasks can be more efficiently performed in batch mode, especially when dealing with large volumes of data. Also, by aggregating and processing data in batches, organizations can optimize performance and reduce the overhead associated with handling individual transactions or real-time updates. Batch processing allows for parallelism, optimization, and resource allocation strategies that can improve overall system efficiency.

**Additional Reading**

[](https://www.notion.so/150915a5a537804f9a38cd5e2d1a281d?pvs=21)

**Standalone Databases and Distributed Databases**

As their name suggests, a standalone database resides on a single server or machine and is managed as a single unit. It consists of a single instance of the database management system (DBMS) running on a single server, handling all data storage and processing tasks locally. On the other hand, a distributed database spans multiple servers or nodes, with each node containing a subset of the data. Data is distributed across multiple nodes, and the database management system coordinates data access and processing across the distributed environment.

To better understand the difference between these two types of databases, let’s start by looking at two real-life examples of data models.

**Advertiser and Ads Data**

Let's consider a database that lets advertisers manage their campaigns. There are 3 entities in the picture, advertiser, campaign, and ad. An advertiser entity can have multiple campaigns running. Each campaign entity can have multiple ads running.

**Side Note**

Note how the relationship between a) Campaigns and Advertisers and b) Ads and Campaigns is maintained. Since it is a many-to-one relationship, the parent ID resides in the child table.

This simple database system allows advertisers to create, update and view their campaigns and ads.

**User Metadata**

Now consider a totally different system that simply shows information about a User and allows the User to edit these free text fields. Users can also add miscellaneous free data to their entity object. This is what the model looks like:

Note that this system simply allows Users to view their Data given an ID and allows the User to update their data given their ID.

**Comparing the Models**

With these two fairly different data models in mind, let's analyze a few things.

**Schema, data model and flexibility**

Note that the first example has data in structured tables with rows and columns, following a predefined schema. Data relationships are established through foreign key constraints. The model is also a lot more strict and less flexible, any attribute that needs to be added would mean that you would need to change the database schema.

On the other hand, although the second example has *some* structure, it is a lot more “schema less” meaning that user metadata could have any arbitrary information, different users could have different keys and keys could be added and removed arbitrarily.

**Query patterns**

The first system would need to address a lot more complex queries that involve joins. For example, “give me all ads of this advertiser” or “give me all ads of this campaign ID”. This would mean that we would have to join data from multiple tables using the foreign key constraints.

For the second example, the queries are a lot more **“point queries”**. Which means the only real query constraint is the ID. All reads and updates are done to this single object using the ID without using any joins.

**Additional Reading on Data Models**

[Relational Model vs. Document model:- Chapter 2. Data models and Query languages, Designing Data Intensive Applications.](https://www.notion.so/Relational-Model-vs-Document-model-Chapter-2-Data-models-and-Query-languages-Designing-Data-Int-150915a5a53780689d12e49fa53e6665?pvs=21)

**Defining Relational and Non-Relational Databases**

The two different use cases explain how two different types of database systems work and are used. With these two examples in mind, let’s jump in to see how relational databases and non-relational databases work.

**Relational Databases**

Relational databases organize data into structured tables with rows and columns, adhering to a predefined schema. Tables represent entities or relationships in the data model, with each row representing a record and each column representing a specific attribute or field. Every table complies with a predefined schema and if any columns need to be added then the schema change needs to be applied beforehand on the entire database table.

Data is stored in **tables** with strict **rows** (records) and **columns** (attributes), adhering to a **schema** (predefined structure).

Relational databases enforce data integrity constraints, such as primary key, foreign key, unique key, and check constraints, to maintain data consistency.

Integrity enforces rules like **primary keys**, **foreign keys**, and **unique constraints** to ensure data consistency.

Usually, relational databases scale less than non-relational databases, and are less performant (think about a) Transactional Support, b) all the integrity checks that need to be made after every transaction). At that cost, they provide high data integrity and high reliability and better support for joins.

Use SQL (Structured Query Language) for **complex joins**, aggregations, and relationships between data.

For querying, relational databases usually use SQL style query languages.  We will look at Relational Databases with more depth in the next chapter.

**Transactional Support**: Follows **ACID properties** (*Atomicity, Consistency, Isolation, Durability*).

**Joins**: Excellent for handling **relationships** between entities (e.g., *many-to-one or one-to-one*).

**Reliability**: Ideal for scenarios requiring **strict consistency** (e.g., *banking or e-commerce transactions*).

**Limitations**:

- Less scalable for massive datasets.
- Schema changes can be challenging.

**Examples**: MySQL, PostgreSQL.

**Non-Relational Databases (e.g. Key-Value Stores, Document Databases)**

These databases don't use the tables, fields, and columns structured data concept from relational databases. NoSQL databases were developed to suit the second example. When the data is such that it doesn't need to be tied down by strict schemas, and the flexibility of adding keys is wanted, it is an attractive choice. Also, when there are no relations between different entities and queries are frequently “point” queries, it could be a good option to go with non-relational databases.

Schema-less, flexible storage models (e.g., key-value, document, column-family).

Non-relational databases generally scale more easily than relational databases and thus can achieve higher throughput. They do well with large amounts of data and are more performant (because of no integrity or schema checks). Of course, at that cost, they often don't provide transactional support (we will speak about this later) and are less performant for queries with joins.

Easily scales horizontally for large datasets and high throughput.

Handles unstructured or semi-structured data; easy to add fields.

Faster for simple operations (e.g., point lookups), no schema or integrity constraints.

Like mentioned above, non-relational databases also don't do so well with relationships (one to one and many to one). If your system has entities that are usually fully loaded at once and somewhat isolated (not connected with other entities), then this is a good option to go with.

Not ideal for one-to-one or many-to-one relationships.

Often lacks full ACID compliance.

Struggles with joins or multi-entity.

**Examples**: MongoDB (**documents**), Redis (**key-value**), Cassandra (**wide-column**).

**Best For**: Isolated entities, real-time analytics, caching, large-scale applications.

**Key Insight**: Use when data relationships are minimal and scalability is critical.

**Relational Database**

**Strict Schemas/Data Model**

As mentioned before, relational databases ensure that each database table has a predefined schema. Every column has a data type. Ids are usually primary keys and relationships are maintained with the help of foreign keys. Whenever new attributes (columns) need to be added in the database, the schema has to be changed beforehand and those schema changes need to be applied to the database before that new column can be used.  Based on the requirements of your system, here are some of the pros and cons of having a strict schema.

Each table has a fixed structure, with **columns having specific data types**.

Tables are linked using **primary keys** and **foreign keys**, enforcing **referential integrity**.

**Adding new columns requires updating the schema** across the database.

**Pros**

- **Clean data, reduced possibility for corruption**. Having data types for each column means that the database will make sure that, in general, garbage data will not enter the db. The schema ensures data integrity and consistency, preventing invalid or inconsistent data from being stored in the database.
    - Schema enforces data validation.
    - Prevents garbage data (e.g., storing a string in a numeric column).
- **Data organization and clarity**. A structured database table is a lot easier to understand and interact with in code.
    - Structured tables make relationships and data easier to understand.
    - Predictable format simplifies coding and querying.
- **Data Integrity.** Having constraints for every column enable having data integrity. For example, Not null columns will make sure there is no null data entered. Foreign key constraints also make sure that the values entered as foreign keys are valid rows in the foreign table. (This is called **Referential Integrity**).
    - **Constraints**:
        - `NOT NULL`: Ensures columns always have a value.
        - **Foreign Keys**: Guarantees linked data exists (e.g., valid user IDs in a transactions table).
    - Maintains consistency between related tables.
- **Query Optimization and performance**. Since the database knows so much about the table, it can optimize query execution plans and generate efficient queries internally, reducing the time and resources required to retrieve and manipulate data.
    
    For example, it can use indexed columns to drastically eliminate rows and cut down the search space.
    
    - **Indexes**:
        - Accelerate searches by creating a "shortcut" for queries.
        - Great for columns that are frequently searched or used in joins.
    - **Query Execution Plans**:
        - Database optimizes the retrieval process using knowledge of schema, constraints, and indexes.
        - Results in faster data access and efficient resource use.
        

In an **e-commerce application**:

- **Users Table**: Schema ensures user IDs are unique (`Primary Key`).
- **Orders Table**: References user IDs (`Foreign Key`) to enforce valid relationships.
- Queries like "fetch all orders for a user" are highly efficient due to structured data and indexed columns.

**Cons**

- **Lack of flexibility.** Having a strict schema means that every new addition of columns needs a schema change applied to the database. This makes it hard for cases where attributes keep changing frequently.
    - **Schema-bound**: Adding new columns requires schema updates.
    - In dynamic systems (e.g., social media with ever-changing metadata), schema rigidity can hinder adaptability.
- **Performance loss on schema updates.** When schema changes alter the table, it could mean downtime since the table will be “locked”.
    - Schema changes (e.g., adding/removing columns) may **lock tables**, preventing reads/writes during updates.
    - This can lead to **downtime** in high-traffic systems.
- **Query Performance.** It's understandable that the database is doing a lot of work and checks while writing data (constraints, datatype checks, referential integrity checks, etc.), which means in general queries, specially write queries in relational DBs are much slower than in non-relational DBs.
    - Relational databases perform extensive checks during writes:
        - **Constraints validation**: Ensures data adheres to rules (`NOT NULL`, `UNIQUE`, etc.).
        - **Data type enforcement**: Prevents invalid data entries.
        - **Referential integrity checks**: Confirms foreign keys reference valid rows.

These operations ensure reliability but increase write latency compared to non-relational databases.

**Transactions/ACID**

Lets see an example of a banking system. Users A and B have bank accounts and user A transfers some money into account B. At a high level, here are all the database operations that happen behind the scenes:

1. Check if A has the balance
2. Deduct money from A’s balance
3. Add money to B’s balance
4. Check if B’s account is correct

Now imagine that after step 2, the database goes down for some reason. It would be detrimental to have the balance deducted from A’s account and not added to B, correct? This is where the concept of a transaction comes into play.

In the context of a relational database, a transaction refers to a logical unit of work that consists of one or more database operations (such as inserts, updates, or deletes) that are executed as a ***single, indivisible*** unit. This means that either **all the operations happen** or **they all don’t**. The concept of a transaction is fundamental to ensuring data integrity, consistency, and reliability in relational databases.

Transactional support is one of the key aspects of relational databases and something that we must consider strongly when selecting the right kind of database for our system. If there are cases where we want all or nothing, relational databases offer a lot more native support for transactions than a non-relational databases

The key aspects of a transaction are **atomicity, consistency, durability, and isolation**. This is where the acronym **ACID** comes from. Let's look at each of them briefly.

1. **Atomicity**
    1. Atomicity ensures that all operations within a transaction are executed as a single, indivisible unit. Either all operations in the transaction are **completed successfully, or none of them are**.
    2. If any operation within the transaction fails or encounters an error, the entire transaction is rolled back, and the database returns to its previous state (i.e., all changes made by the transaction are undone).
2. **Consistency**
    1. Consistency guarantees that the database remains in a valid and consistent state before and after the execution of a transaction.
    2. Transactions must adhere to all integrity constraints, such as primary key constraints, foreign key constraints, and unique constraints, to **ensure the integrity of the data**.
3. **Isolation**
    1. Isolation ensures that the changes made by one transaction are isolated from the changes made by other concurrent transactions.
    2. Concurrent transactions execute as if they were isolated from each other, **preventing interference and maintaining data integrity**.
    3. Isolation levels, such as Read Uncommitted, Read Committed, Repeatable Read, and Serializable, define the degree of isolation between concurrent transactions.
4. **Durability**
    1. Durability guarantees that the changes made by a committed transaction persist even in the event of a system failure or crash.
    2. Once a transaction is committed, its changes are written to durable storage (such as disk) and remain intact, even if the system crashes or loses power.

**Data Modeling with Relational Database**

In the previous unit, the Advertiser and Ads data system was a good example of an object model. Lets look at another data model known as the “Graph Data Model” and how we can use relational databases to implement this data model to represent nodes and edges

**Graph Data Model**

Consider a social network where users have other friends, followers and posts.

- Users can author Posts. A post can have only one author.
- Users can be friends with other users. This is a bidirectional edge, which means if A is a friend of B, B is a friend of A too
- Users can follow other users. This is a unidirectional edge, A following B does not mean that B follows A too.

**Modeling Graphs in Relational DBs**

**How would we model this graph in a relational DB?**

First, every node will have its own table. So here, we would have 2 node tables, User and Post.

Next, the User <-> Post mapping is a one to many relationship and therefore the post table can have a ***foreign key*** for its author pointing to User ID. Following the above two points here is what the User table and Post Table would look like:

The graph data model is a way to represent relationships (edges) and entities (nodes). Here's how this model is applied to a relational database using a **social network** example:

**Nodes as Tables**

**Users**: Each user is a node, so we use a `User` table.

- **Columns**:
    - `ID` (Primary Key)
    - `First Name`, `Last Name`, `Country`, `Currency`

**Example**:

```sql
sql
Copy code
| ID  | First Name | Last Name | Country  | Currency |
|-----|------------|-----------|----------|----------|
| 1   | Alice      | Smith     | USA      | USD      |
| 2   | Bob        | Johnson   | Canada   | CAD      |

```

**Posts**: Each post is another node, linked to a user by a foreign key `Author_id`.

- **Columns**:
    - `ID` (Primary Key)
    - `Author_id` (Foreign Key referencing `User.ID`)
    - `Post Text`

**Example**:

```lua
lua
Copy code
| ID  | Author_id | Post Text           |
|-----|-----------|---------------------|
| 1   | 1         | Hello, world!       |
| 2   | 2         | Exploring Canada!   |

```

| Column 1 | Column 2 | Column 3 | Column 4 | Column 5 |
| --- | --- | --- | --- | --- |
| ID | First Name | Last Name | Country | Currency |

| Column 1 | Column 2 | Column 3 |
| --- | --- | --- |
| ID | Author_id (Foreign Key to ID column of the User table) | Post Text |

**Edges as Tables**

**User Relationships**:

- Represent **many-to-many relationships** (e.g., friendships and follows) in a separate **Edge Table**.
- **Columns**:
    - `From_ID` (Foreign Key referencing `User.ID`)
    - `To_ID` (Foreign Key referencing `User.ID`)
    - `Edge Type` (e.g., `FRIEND`, `FOLLOWS`)

**Example**:

```vbnet
vbnet
Copy code
| From_ID | To_ID | Edge Type |
|---------|-------|-----------|
| 1       | 2     | FRIEND    |
| 2       | 1     | FRIEND    |
| 1       | 3     | FOLLOWS   |

```

The friends and following edges represent many to many relationships. One way of representing many to many relationships is to have a ***separate edge table*** to represent those edges. The edge table can have one column as the **from_node**, the other one as **to_node** and one column that mentions the edge type. For example with respect to the diagram above we could have:

| Column 1 | Column 2 | Column 3 |
| --- | --- | --- |
| From_ID | To_ID | Edge Type |
| USER ID 1 | USER ID 2 | FRIEND |
| USER ID 2 | USER ID 1 | FRIEND |
| USER ID 1 | USER ID 3 | FOLLOWS |

There are some discussions on whether to have a separate table for each edge type or whether to keep them in the same table with a column that denotes the edge type. The answer depends on a lot of things such as how much we expect the data to grow and whether one table might become too big.

- One general consensus is that if we expect to introduce new edge types all the time, have a separate table for each might become a big overhead in terms of schema changes and so on.
- If each edge table has a lot of metadata that itself might need structure, then maybe it's okay to have one table per edge type.

**Design Considerations**

1. **One Table for All Edge Types**:
    
    Use a single table for relationships (e.g., `Edge Type` column distinguishes between `FRIEND` or `FOLLOWS`).
    
    **Pros**: Easier to manage schema changes when adding new edge types.
    
    **Cons**: A large table can become less performant as data grows.
    
2. **Separate Tables for Each Edge Type**:
    
    Create a different table for each relationship type (e.g., `Friends Table`, `Follows Table`).
    
    **Pros**: Allows better structuring if each relationship type has its own metadata.
    
    **Cons**: Inflexible for frequently changing or new edge types.
    

The purpose of this excerpt is to explain how to model a **Graph Data Model** (nodes and edges) in a **Relational Database** using a **social network** example, with a focus on:

1. **Nodes as Tables**: Represent entities (e.g., Users and Posts) as relational tables with columns for attributes and foreign keys for relationships.
2. **Edges as Tables**: Represent relationships (e.g., friendships and follows) as separate tables to handle many-to-many connections, using columns for source, target, and relationship type.
3. **Design Considerations**
    
    **Single Table for All Edges**: Simplifies schema management but may affect performance with large datasets.
    
    **Separate Tables for Each Edge Type**: Offers better structure for metadata-heavy relationships but is less flexible for frequent schema changes.
    

**Non-Relational Database**

We saw an introduction to non-relational databases in the previous unit. Let’s dig into some more concepts pertaining to these systems in a little more detail.

**Key Concepts and Example**

**Schema-Less/Flexible Data Model**:

**Structure**: Non-relational databases, like key-value stores or document databases, do not follow strict schemas. The "value" is a blob (e.g., JSON), allowing the structure to evolve dynamically.

**Example**: A user metadata system:

```json
json
Copy code
{
  "User_ID_1": {
    "First_name": "Foo",
    "Last_name": "Bar",
    "Location": "SF",
    "Interests": ["Tech", "Music"]
  }
}

```

New fields (like "Interests") can be added without modifying a predefined schema.

**Why Use It?**

- New fields can be added anytime without downtime or schema migrations.
- Perfect for use cases like user profiles where fields change often.

**Advantages**

1. **Flexibility**:
    - Rapidly evolving systems can adjust without requiring schema updates, reducing downtime and accelerating development.
    - Ideal for unstructured or semi-structured data.
    - **Example**: Product catalogs with varying attributes like size, weight, or color.
2. **Performance**:
    - Faster writes since there are no schema validations or foreign key checks.
    - They are optimized for fast insertions.
    - **Example**: Logging systems where speed is critical.
    
    **Not Good For**:
    
    - **Complex relationships** (e.g., joins between tables).
    - **Strong consistency or transaction-heavy systems** (e.g., banking).

**Flexible Data Model/Schema-less**

In general, non-relational databases work like key-value stores. They do not store data in rows and columns like the way relational databases store data. Rather, the value is usually a data blob, meaning it can have a fuzzy structure that changes from time to time. This way, the data is not tied down to a strict schema.

Let's look at an example. Let's assume, we have a database that stores metadata about a user. Metadata could be first name, last name, location, and email.

The system allows users to view their data, edit their data and also new users to add their data.

Now, we can imagine that with a system like this, there will always be an opportunity to include more and more metadata. For example, it's possible that a little later down the line we add date of birth, interests, educational background, professional background, etc.

Now if this system was modeled with a relational database, then every time a new field was added, we would have to change the entire schema of the table and apply the schema to the database before it could be used. This could mean slower development time, database downtime, and if it happens often enough, it could really slow things down (both development-wise and performance-wise). Therefore, in this case, a good choice would be to have a key-value store or a document database whose key would be the ID of the user and value would be a JSON block of all the information.

```
{
  "User_ID_1": {
    "First_name": "Foo",
    "Last_name": "Bar",
    "Location": "SF",
    ...
  },
  ...
}

```

In a schema-less model:

1. **Key** acts like a unique identifier (like a locker number).
2. **Value** is the data stored (like the contents of the locker), often represented as a JSON object or blob, allowing dynamic and varying structures.

Imagine a **relational database** as a vending machine. Each slot in the machine is predefined (rows and columns), e.g., slot "A1" for "Chips". If you want to sell a new product, you'd need to redesign the machine to add a new slot.

Now imagine a **schema-less database** as a storage shelf. You can put any item in any spot without modifying the shelf. Need to store chips today and umbrellas tomorrow? No problem—it’s flexible and quick.

**Relational Database Challenge**: Schema change required—time-consuming and may cause downtime.
**Schema-less Solution**: Simply add new fields to the JSON blob for that user without impacting others.

**Example Scenario:**

- A database tracks user metadata (name, location, etc.).
- Initially: Only basic fields like name, location.
- Later: New fields like date of birth or professional background are added.

**Pros**

- **Flexibility of data model**: We are not bogged down by strict schemas and painful schema changes. Fast evolving systems benefit hugely from this. Evolving systems benefit as developers can add new fields without schema migrations.
    
    Analogy: It’s like using a whiteboard where you can add or erase notes easily, compared to a printed page where changes require reprinting.
    
- **Faster writes:** Non-relational databases perform better in general during writes. Imagine not having to validate the data schema, data types, foreign keys, etc. No schema validation means data gets written quickly.
    
    Analogy: Like skipping security checks at the airport—faster but less regulated.
    

**Cons**

- **Increased possibility of corruption**: Since the data is simply a blob, it's possible for bad data to get in if not handled by the application code correctly. For example, at a DB level, there is no check for what the JSON keys are.
    
    No enforced schema means invalid or inconsistent data may sneak in.
    
    Analogy: Like leaving a shelf unorganized; if someone puts a shoe in a cereal box, you’ll only notice later.
    
- **Less clarity and data organization:** If you look at a relational table, just a glance at the schema can tell you what the data would look like. However, in a non-relational DB, that's very hard to tell because in theory, every JSON blob of different data points could have a different set of keys in them.
    
    Unlike relational schemas, where data relationships are clear, schema-less databases can appear chaotic.
    
- **No support for join queries**: Since the data is unstructured, if your system has queries that need to join multiple non-relational tables, the query performance could be poor. In the above example, let's assume you have another key-value store of locations keyed by ID, like this: Now if the User metadata has a location pointing to this ID (instead of the name string), then for a query like “Give me all users in San Francisco”, you would have to parse through all the users’ information and one by one, match it with the above table to get the result. This would be something a relational database would do much better with joins.
    
    **No optimized joins**: You manually match related data (e.g., user locations from another table).
    
- **No support for range and comparison queries**. Imagine we had a query that wanted to find all users born after a certain date. NoSQL databases cannot optimize this query and will have to scan all the rows to get the result, whereas relational databases would have no problems doing this optimally.
    
    **Limited range queries**: Scanning all data to filter (e.g., finding users born after 2000).
    

**Question: Does this mean that we can never have validation on the structure of data when we use non-relational databases?**

We can, but that will most likely have to be done in the application code. The application should validate the structure while writing or reading from the database and then handle errors accordingly. This is why these systems are also called as “**Schema on Read**”.

**Example**: Imagine a library without a catalog. When a user asks for a book, the librarian must check every shelf to find it. Validation occurs at the time of the query, not during storage.

**Transactional Support/BASE**

In general, document databases or non-relational databases are meant for single object reads and writes. They are built for blazing fast writes and for scaling horizontally. This means that generally they are not built to support “transactions” across objects or multiple operations. Supporting transactions consumes time and resources. Think about starting a logical transaction and then all the work you need to do for locking and to make sure all the operations within the transaction are complete and if not, all the rollback operations that need to be done, etc. Non-relational databases work best for use cases where transactional support is not needed across objects and operations.

Therefore, non-relational databases do not inherently support the ACID properties that relational databases do.

Instead of ACID, another term has been coined for non-relational databases called **BASE**. Base stands for **basically available, soft state and eventual consistency**. 

- Atomicity: Transactions are all-or-nothing.
- Consistency: Database remains valid after transactions.
- Isolation: Concurrent transactions don’t interfere.
- Durability: Data is saved permanently.

**Non-Relational Databases (BASE):**

- **Basically Available**: The system is always responsive.
- **Soft State**: Data may not be immediately consistent.
- **Eventual Consistency**: Consistency is achieved eventually, not instantly.

**Reason for BASE**: NoSQL databases prioritize speed and scalability over strict consistency, fitting scenarios where perfect data synchronization isn't critical (e.g., social media feeds).

**Question: So is it not possible to support transactions over multiple objects in a non-relational database?**

No, it is totally possible to have transaction support in a non-relational database. However, since they were not made with an intention to support transactions, they usually don't have “native” support for this. Therefore, typically the way to do that would be to run another layer of middleware on top of the non-relational database that does the transaction management for the application.

For example:

- **OMID** is a system that helps support transactions on HBase (which natively does not support multi-object transactions)
- Some non-relational databases, like **MongoDB**, have introduced support for multi-document transactions.

So now that we have seen relational and non-relational databases, let us list out some of the things we need to look out for while selecting the right type of DB for our system.

**Question: Is it true that “most” simple use cases can be implemented using relational databases?**

In general, it is mostly true that most simple use cases will work with relational DBs. However, the key thing to note here is that relational DBs might not be the best fit for your system and in some cases might be ***extremely suboptimal*** and lead to a degraded and poor performance.

If we wanted to understand this briefly, imagine we have a system that allows users to simply view and update their own names. This system needs to be able to support billions of users and a very high write/update traffic. The system also needs to scale horizontally to support more and more traffic.

Now, if we use a relational database, we are trading off high write performance in order to get transactional support. However, this system does not need transactional support (no multi object writes per operation) and therefore we would be giving away write performance for something we don't really need at all.

Secondly, this simplistic system does not *need* data to be stored in structured columns (since it's simply just the name) and read queries don't require complex joins (since you only read by user ID). Therefore, using relational databases don't give us any more advantage with read queries and unnecessarily might slow us down with unneeded strict schemas and system downtime when schema changes need to be propagated to the database.

**Selecting Between the Two**

So the question is, how do we select the right storage system for our system? To get a more thorough understanding of this, we need to answer a set of questions about the system in a way that tells us if we can benefit from the pros of each option and are okay to trade off the cons of that particular option. Here are some of them.

If your queries frequently involve joins or analyzing relationships between entities, go with a relational DB.

 If the data schema is dynamic and unpredictable, non-relational DBs provide the flexibility you need.

**What are the entities that we will be storing and retrieving from the database? Are these entities strongly connected to each other? In other words, do they have relationships with each other?**

In the above example, the only entity our system had was the user. Users have no relation to each other, and each user simply updates or views their own information.

Contrast this with the system that we discussed previously, the advertiser and ads data system, where we see a strong relationship among entities.

Therefore, if entities in a database have relationships with each other, a relational database would be a better fit for the system. Here, the advertiser system would benefit from a relational database whereas the user meta data would not really benefit from it.

**Is the structure of the data your system stores continuously evolving? Do you foresee more fields/attributes being added and removed all the time?**

We can imagine that the advertiser and ads data system is fairly structured. We might add attributes to the entities from time to time but it is not going to happen frequently. On the other hand, it's possible that the user metadata system keeps evolving in the data it stores. It may store names today but in the near future it's totally possible that we add more data like location, email, phone, birthdays, etc. Such systems that change continuously might not want to be bogged down by strict predefined schemas, and slow schema changes that cause downtime. They benefit better with non-relational databases.

**Even if there are relationships between entities, do we need strong transactional support while editing and reading these entities? Are operations done on several connected entities together?**

Now let's assume we have a social network where users can be friends with other users.

https://codahosted.io/docs/dD_gKH7E1I/blobs/bl-EHONoCpIUi/abe84a8efdf86a183037840a8703fa42afbe57a13ff047be68bd8a35a1ce1b069a4dc4b9a1699ef09bd49971d25eb0029a5eb91d20876c022019afeb39a294d3d2ff6b66736858bec0f8f0e76c3acc4c441937a8f16b206e10eac2e2aa49d9247cda4a62

Here, there are entities that have relationships with each other. It may be tempting to straight away choose a relational DB. However, we will probably ***never*** see a case where multiple of these entities and relationships are created as ***part of the same operation.*** Users usually create their own entities, and then friendship relations are created individually and separately at a later point in time.

To be more clear, we probably will not have cases where we want to create multiple users as well as friendship connections *in one go*. Furthermore, we won't have atomicity constraints where the entities and relationships all need to be created at once and if one relationship fails to be created, all the other entities need to be rolled back/deleted. Such systems benefit from non-relational databases because they don't need ACID support and can also benefit from much faster writes and ***much better scaling,*** which is what we would want from a social network.

On the other hand, the advertiser system may require that a campaign and a set of ads under it are all created ***in one operation.*** Failure to create the campaign entity should result **in no ads** getting created. Also, advertiser entities don't scale as fast as user entities in general, and so we are not bogged down by slightly slow writes. On the other hand, we do need the ACID support and therefore a relational database in general would do well here.

- **Entities and Relationships**: Users and friendships between users.
- **Operation Characteristics**:
    - Users create their profiles individually.
    - Friendships are established later and independently.
    - There is no need to batch-create multiple users and their friendship links in a single operation.
    - A failure to create a friendship link doesn’t require rolling back any user entities.

**In this case**:

- There’s no dependency on **strong transactional support** for operations spanning multiple entities.
- **Non-relational databases** excel here because:
    1. **Faster Writes**: The system avoids the overhead of ACID compliance.
    2. **Scalability**: Social networks often need to scale horizontally to accommodate billions of users and high write traffic.
    3. **Dynamic Schema**: User profiles may evolve (e.g., adding hobbies or custom statuses), which schema-less systems handle gracefully.

**What kind of read queries is the system going to support? Do we have “point” queries that don't require complex joins, or do we have queries requiring joins all the time?**

Point queries are queries that have simple constraints and don't require us to join data from multiple tables to get the results. For example, in the user metadata systems, all that the user can do is read their data given their ID and update their own data with the help of their ID. There is no real need for one user to query other users' information. Non-relational databases would work for such systems because they support blazing fast point queries.

On the other hand, the advertiser system does need to answer questions like “give us all the campaigns and ads of this advertiser”. Automatically, this would mean that we would have to join data from the campaigns and ads tables. In general, non-relational databases perform very poorly with joins. Remember, there is no structured set of columns and no concept of foreign keys, so the database wouldn’t be able to perform joins optimally. On the other hand, relational databases do really well with joins since the database knows the structure of the table well and can benefit from the indexes it has created for primary and foreign keys (We will talk about indexes in future chapters in more detail). Such cases benefit greatly from relational databases.

- **Entities and Relationships**: Advertisers, campaigns, and ads.
- **Operation Characteristics**:
    - A campaign creation might involve batch-creating multiple ads.
    - Failure to create a campaign should roll back all associated ads.
    - Advertisers and their campaigns have tightly connected data relationships.

**In this case:**

- **Transactional atomicity** is critical:
    - You don’t want partial data (e.g., campaigns created without ads) due to failures.
- **Relational databases** excel here because:
    1. **ACID Properties**: Ensure the atomicity, consistency, isolation, and durability of these operations.
    2. **Complex Queries**: Advertisers may want reports combining data from campaigns and ads, requiring optimized joins and foreign key relationships.

**Point Queries**

- These are simple queries where data is retrieved or updated by a unique identifier.
    - **Example**: "Get the user profile for user with ID X" or "Update the location for user with ID Y."
    - No joins or complex conditions are involved.

**Non-relational databases** are optimal for point queries:

1. **Blazing Fast Reads**: Keys are **indexed** for direct access.
2. **Schema-less Flexibility**: Each user’s data is self-contained, so schema changes won’t disrupt queries.

**Planning for the Future**

One pattern that we may see in the above Q&A is that we not only look at the current state of the system, but also plan a bit for the near future. We need to know briefly how the system is going to evolve over time, the query patterns that might change over time, the data that might change over time. Just looking at the current state of the data might give us a one-sided view, which might not be optimal.

**For example**, the user metadata system simply stores the names of users today. Knowing that, we may decide to go with a relational database. However, if we know that it's possible that more and more data is going to be added to this metadata store and that the fields are going to be added/removed very frequently, we might want to rethink the decision. Relational DBs will soon slow us down with slow schema changes and frequent application downtime, and therefore we might want to change our decision to using a non-relational database.

**Consider the Holistic Impact**

For the ease of understanding, we looked at each question individually and in isolation, but in practice, we need to look at all questions together holistically. For example, it's possible that answering one question might lead us to choose relational databases but in answering the next one, non-relational databases come out as the optimal choice. In such cases, we must:

1. Enlist all the pros and cons of each option with reference to the questions.
2. See if there are any non-negotiable requirements that come along the way. For example, if transactional support is absolutely needed, and we cannot give that away, even if the fields of the data keep changing very dynamically we might have to select relational databases and accept the disadvantage of frequent schema changes.
3. Finally, go with the option that maximizes how the system will benefit with the pros of that option and minimize the disadvantage we will face with the cons of the option. In all honesty, this is a world of trade-offs, and therefore we will very rarely have a system-database pair where we benefit from all the pros of the database and have no disadvantage from the cons of the database.

**NewSQL Databases**

Apart from the two major types of databases discussed previously, there is also a new emerging trend of NewSQL databases. NewSQL databases are like a middle ground between traditional relational databases and non-relational databases. They offer the scalability and performance benefits of NoSQL databases while retaining the ACID compliance and SQL compatibility of RDBMS. We will look at ACID compliance in detail in the next chapter.

**Examples include**: CockroachDB, YugabyteDB, TiDB, and MemSQL. Read about these in more detail if you’re curious.

**Question: It seems like NewSQL databases are magic. They give us the pros of both relational and non-relational databases and seem to have no cons. What's the catch? How can it be possible that a solution gives us both sides of the trade-offs?**

Well, it's true that NewSQL does give us the transactional support (ACID) of relational databases, the SQL support for relational databases while giving us the ability to scale horizontally like NoSQL databases. However, the key thing to note is that they meet us in the middle. They offer a balance between relational and non-relational features, and they may not be the best fit for every use case.

**For example**, they may be better than relational databases in scaling, but they may not always match the scalability and performance characteristics of specialized NoSQL databases. Organizations with extreme scalability or performance requirements may still prefer specialized NoSQL databases tailored to their specific needs. Extreme write-heavy workloads might still be more optimized with NoSQL databases.

Another point is that in general, NewSQL databases still maintain a relational data model, which means that generally they still require a predefined schema for data organization and may not be strictly schema-less like some NoSQL databases. Which means that while there are ways of getting more schema-less with New SQL compared to Relational databases, systems that require extreme flexibility might still not benefit from NewSQL Databases.

While NewSQL databases scale better than traditional RDBMS by using distributed architectures, they might not achieve the **same extreme scalability or performance** as specialized NoSQL systems in certain scenarios:

**Write-Heavy Workloads**: For workloads with intense, high-throughput writes (e.g., logging massive IoT data streams), NoSQL systems like Cassandra, optimized for eventual consistency, might outperform NewSQL.

**Ultra-Low Latency**: In systems that prioritize milliseconds of response time over strict consistency, NewSQL's adherence to ACID properties might introduce additional overhead.

NewSQL databases typically adhere to a relational data model, requiring **predefined schemas**:

This works well for structured data but **limits flexibility** for systems needing frequent schema changes or working with unstructured data. **For example**:

Dynamic applications that frequently modify their data models might find NoSQL's schema-less flexibility more convenient.

Content management systems handling unstructured data like images, videos, or JSON might favor NoSQL.

**Distributed Architecture :** Achieving ACID compliance in a distributed system is complex. 

This can lead to:

**Higher operational costs**: Managing and maintaining a distributed NewSQL cluster is more resource-intensive.

**Latency trade-offs**: Consistency across nodes might result in higher latency for some operations compared to NoSQL systems with relaxed consistency models.

**Cost of Adoption**

NewSQL databases often involve **additional operational costs** due to their distributed nature, requiring multiple nodes and resources for deployment.

Teams may face a **learning curve** in understanding the nuances of distributed systems, even though SQL itself remains familiar.

Think of NewSQL as a versatile **crossover vehicle**: It blends the comfort of a sedan (RDBMS) with the off-road capability of an SUV (NoSQL). However, it might not perform as well as a dedicated off-roader in rugged terrain or match the efficiency of a sedan on a highway. It’s designed for balance, not specialization.

**Vector Databases**

Vector databases are a relatively new concept, and they’ve quickly become very popular because of the unexpected AI boom in a short time. Vector databases specialize in storing vectors, which are mathematical representations of data in a high-dimensional space. AI algorithms rely heavily on “nearest neighbor search” and vector databases.

Vector databases are purpose-built to store and query **vectors**, which are numerical representations of data in a high-dimensional space. These databases have risen to prominence due to their ability to handle tasks central to modern AI applications, such as similarity search, recommendation systems, and natural language understanding.

Make it easier to represent large data in terms of vectors

Implement approximate nearest neighbor algorithms to help implement AI functions like similarity search, providing recommendations and so on.

A **vector** is essentially a list of numbers that captures the essence of a piece of data.

**Example**: A vector representing a sentence might encode its meaning, tone, and context into, say, 512 dimensions.

These vectors are generated using AI models like word embeddings, image encoders, or recommendation algorithms.

**Why use vectors?**

Unlike raw data (text, images, etc.), vectors allow **quantitative comparison**. For instance, two similar sentences will have vectors that are “close” to each other in a high-dimensional space.

Traditional databases are inefficient for managing high-dimensional vectors. Vector databases optimize this process by:

1. **Efficient Storage**: Organize vectors for fast retrieval.
2. **Similarity Search**: Implement **Approximate Nearest Neighbor (ANN)** algorithms, enabling:
    - **Quick similarity comparisons**: Finding the closest vector (or vectors) to a query.
    - **Example**: In a recommendation system, a user’s vector can help find similar items (movies, products, etc.).
    - **Scalability**: Handle millions or billions of vectors efficiently.
3. **Integrating AI Functions**:
    - AI models rely on finding patterns and relationships between vectors. Vector databases enable:
        - Recommendation engines.
        - Semantic search (search by meaning, not exact text).
        - Image or video similarity matching.

**Example Algorithms: Approximate Nearest Neighbor (ANN)**

Finding the nearest vectors in high-dimensional space is computationally expensive due to the **curse of dimensionality**. ANN algorithms make it practical by:

1. Using **heuristics** instead of exact calculations.
2. Organizing data with techniques like:
    - **Hierarchical Navigable Small World (HNSW)** graphs.
    - **Product quantization (PQ)**.

These methods speed up searches while keeping accuracy high, making real-time AI applications feasible.

**How Vector Databases Differ**

**Versus Traditional Databases**:

Traditional databases store structured, relational data and focus on exact matches or range queries.

Vector databases focus on **approximation and similarity**, which are computationally expensive in traditional systems.

**Versus In-Memory Tools**:

Many AI applications used in-memory data structures (e.g., Python libraries like FAISS) for vector search. However, vector databases add persistence, scalability, and easier integration into production systems.

**Popular Use Cases**

- **Recommendation Systems**:
    - **Example**: Suggesting similar movies based on user preferences encoded as vectors.
- **Semantic Search**:
    - Query: “Best Italian restaurants near me.”
    - Instead of exact keyword matches, the database finds vectors representing similar concepts.
- **Image and Video Search**:
    - Example: "Find images visually similar to this one."
- **Anomaly Detection**:
    - Outliers in vector space can signal anomalies in financial transactions, health data, or system logs.

**Object Stores**

Let's assume we have a system that needs to store large files, for example photos or resumes. In such cases, object stores are the preferred choice of DB architecture. Object stores, such as Amazon S3 (Simple Storage Service), are a type of online/real-time database designed for storing and retrieving large amounts of unstructured data, typically in the form of objects or files.

Object stores organize data as objects, and each object is stored as a file. File sizes can be as small as bytes or as large as terabytes. Therefore, object stores are highly scalable and are designed to handle large-scale data storage and retrieval efficiently.

Object stores are a specialized type of database architecture designed to handle **large amounts of unstructured data**, such as images, videos, documents, and backups. They have become indispensable as data sizes and usage demands continue to grow exponentially.

As more and more applications and industries are challenged with rapidly growing data requirements, object stores have become more and more ubiquitous, essential and important.

**Key Characteristics of Object Stores**

**Object-Based Storage**

Data is stored as **objects**, which include:

The **data** itself (e.g., the file or binary content).

**Metadata** that describes the object (e.g., file type, creation date, or permissions).

A **unique identifier** (like a key) that makes each object retrievable.

**Scalability**

Object stores are inherently **horizontally scalable**, meaning storage can grow indefinitely by adding more servers or nodes.

This makes them ideal for applications that need to handle terabytes or petabytes of data, such as media libraries or cloud backups.

**Access via APIs**

Objects are accessed through **REST APIs** rather than traditional file systems or relational database queries.

**Example**: Amazon S3 uses simple API calls like `GET`, `PUT`, and `DELETE` to interact with stored objects.

**Cost Efficiency**:

Object storage often operates on a **pay-as-you-use model**, making it cost-effective for storing large amounts of data long-term.

Tiered storage (hot, warm, cold) allows users to balance cost with access speed.

**Data Durability and Redundancy**:

Object stores replicate data across multiple servers and geographic regions to ensure durability and prevent data loss.

They are designed for **high availability**, ensuring stored objects are almost always accessible.

**Why Object Stores Are Critical Today**

**Handling Unstructured Data**:

Most of today’s data (e.g., photos, videos, logs, IoT data) is **unstructured**, and object stores are optimized for such formats.

Unlike traditional databases, which require structured schemas, object stores impose no schema, allowing flexibility in data storage.

**Support for Modern Workloads**:

Applications like machine learning, big data analytics, and content delivery rely on massive datasets stored efficiently.

Object stores allow seamless access to these datasets at scale.

**Global Accessibility**:

Object storage systems are often distributed globally, allowing data to be accessed quickly from any location. This is crucial for applications like streaming services or global SaaS platforms.

**Comparison to Other Storage Models**

**Versus File Storage**:

File storage uses a hierarchical structure (folders and files). It’s less scalable and harder to manage for very large datasets compared to object stores.

Object storage’s flat structure with unique identifiers **eliminates directory complexity and enhances scalability**.

**Versus Block Storage**:

Block storage is ideal for structured workloads **requiring fast, low-latency performance** (e.g., databases or virtual machine disks).

Object storage focuses on **large, unstructured datasets** and provides features like metadata and API-driven access.

**Use Cases of Object Stores**

**Media Storage and Delivery**:

Platforms like Netflix or YouTube use object stores to manage vast libraries of videos and ensure high availability and quick delivery.

**Backup and Archiving**:

Object stores like Amazon S3 Glacier are tailored for cost-efficient long-term data archiving.

**Big Data and AI**:

Machine learning models rely on large datasets, often stored as objects, for training and inference.

**Log Management and IoT**:

Storing and analyzing logs, sensor data, or telemetry data from IoT devices.

**OLTP Systems** (**Online Transaction Processing)**

Most of the examples we’ve seen earlier (advertiser system, user metadata system) are examples of systems where:

1. The **clients are end users**.
2. Typically the clients want to read/update their information, which means the application queries and looks up a small number of records in the database at a time.
3. The queries are real time and have low latency.
4. The queries are interactive and depend on the latest state of the database.

In general if your system has the above attributes, we call it an **OLTP** system. OLTP stands for **Online Transaction Processing**.

**OLAP Systems (Online Analytics Processing)**

Over time, the data we’ve collected has exponentially increased and now there is more and more need for extracting “insights” out of this data. As systems become intelligent and with the advent of AI, the need for data science and analytics has seen a boom. There has emerged an entire new use case of querying databases in order to get more insights rather than just serving the end user.

To give an example, think about queries like “What is the total spend across all advertisers for this quarter” or “Which are the top categories of campaigns across which money has been spent”.

Note that these queries:

1. Typically query a lot of rows as opposed to OLTP systems where you only read and update a small set of rows.
2. The clients of such queries are usually not end users but internal teams of companies trying to get some insights out of this data.
3. The number of queries are much lower compared to OLTP systems that serve many more users, however each query is a lot larger, slower and compute intensive.
4. These systems do not need to get the exact real time state of the database. A lag of a few minutes or even a few hours is usually okay for the use cases they serve.
5. These queries are typically only read queries.

Such systems are called **OLAP** systems. OLAP stands for **Online Analytics Processing.** You may have noticed the stark difference between the query patterns of OLAP and those explained in OLTP systems.

**Question: Can the same database service both OLTP and OLAP queries?**

Broadly, the answer to this is yes. If you have a relational database, you can always use the right “Where”, “Order by” and “Group By” clause in your SQL to get answers to even OLAP queries. However, this is possible only to some extent. Here are some of the limiting factors:

1. OLAP queries in general query tons of rows and therefore consume a lot of the database resources (CPU, memory, etc). Sometimes this may lead to such queries overloading the system and therefore degrading the experience for OLTP queries used by end users. This leads to a **bad user experience**.
2. It's possible that over time, the data size may become so large or the queries can get so complex that typical online databases may not be able to return the result within the stipulated time and with the resources they have. A lot of scaling work might have to be put in (**extra replicas, vertical scaling**) just to make those OLAP queries work which may have some negative side effects on the OLTP queries. This in turn might increase the cost of the system to unexpected levels.

**Resource Contention**

**OLTP Workloads**:

Focus on many **small, fast, and frequent operations** like inserts, updates, and deletes.

Typically require **low latency to serve end users efficiently**.

**OLAP Workloads**:

**Involve complex queries over large datasets**, such as aggregations, joins, and sorting, which are **resource-intensive**.

**Consume significant CPU, memory, and I/O**, often monopolizing resources.

When both workloads run on the same system, heavy OLAP queries can **monopolize** database resources, leading to:

**Slower response times** for OLTP queries, degrading user experience.

**Increased latency for critical user-facing operations**, which can cascade into system-wide performance bottlenecks.

**Data Size and Complexity**

As data grows, OLAP queries may need to process millions or billions of rows.

**Relational databases** designed for OLTP:

Are optimized for small-scale, row-based operations.

May struggle to efficiently handle OLAP's large-scale, column-oriented analytical queries.

**Scaling Challenges**:

**Vertical scaling**: Adding more powerful hardware to handle OLAP queries might **improve performance temporarily but is costly and has diminishing returns**.

**Replication**: Adding read replicas for OLAP can work but **increases complexity and costs. Also, replicas may lag behind, leading to outdated analytics**.

**Query Optimization Challenges**

- OLTP databases are **optimized for quick transactions**, often using row-based storage to quickly access specific records.
- OLAP workloads benefit from **columnar storage**, where only the relevant columns are scanned for faster aggregation and analysis.

**Mismatch**:

- A database serving both workloads may not optimize effectively for either:
    - OLAP queries may **execute slower due to row-based storage inefficiencies**.
    - OLTP performance **may degrade due to the overhead of maintaining indexing or storage structures needed for analytics**.

**Combining OLTP and OLAP in Practice**

**Data Warehouses**

It's evident that there is a new need for a different kind of storage system that allows and optimizes OLAP queries. Such a database that can be used for analytics queries is usually called a **data warehouse**. Here are some of the traits of a data warehouse.

1. Data warehouses are typically **offline databases**. This means that they are not meant for real time interactive queries. They typically don't have the latest state of the database and therefore isn't the full source of truth when querying for the state of an individual entity in the database.
2. Data warehouses are populated by copying over data from OLTP systems **in a batch or stream fashion**. 
    
    Data from multiple OLTP tables are then merged for analytic friendly queries.
    
3. The process of copy over multiple OLTP tables, cleaning the data and transforming the data into a query friendly schema is known as **ETL (Extract-Transform-Load)**.
4. Data warehouses store enormous amounts of data, even historical data. This way they are better suited for analytic queries.
5. Since data is copied over from online databases, data warehouse tables are often **read only.**
    
    Examples of such systems are Amazon Redshift, and Apache Hive.
    

Data warehouses are purpose-built systems for **OLAP (Online Analytical Processing)** queries, enabling organizations to derive insights from vast amounts of data. They cater specifically to analytical workloads and differ significantly from OLTP databases in design and function.

**Offline Nature**

Data warehouses are **not designed for real-time queries or transactional operations**.

Instead, they are optimized for **complex, aggregate queries** over large datasets.

As they do not have up-to-the-minute updates, they **aren’t the definitive source of truth** for current transactional data but **excel at providing historical and trend analyses**.

**Data Integration via ETL (Extract-Transform-Load)**

Data warehouses rely on **ETL pipelines** to populate their datasets:

**Extract**: Data is fetched from OLTP systems and other sources.

**Transform**: Data is cleaned, deduplicated, and reshaped into a schema optimized for analytical queries.

**Load**: The transformed data is stored in the warehouse.

This process allows data from multiple sources to be **merged and aligned**, creating a unified view for analytics.

**Schema Design for Analytics**

Data warehouses use schemas optimized for OLAP queries, such as **star schema** or **snowflake schema**:

**Star Schema**: Central "fact" tables (e.g., sales data) connected to "dimension" tables (e.g., customers, regions).

These designs reduce query complexity and enhance performance.

**Storage of Historical Data**

Unlike OLTP systems that only store current state data, warehouses often store **long-term historical records**.

This enables trend analyses, forecasts, and comparisons over time, critical for decision-making.

**Read-Only Data**

Since data warehouses are not designed for transactional workloads, their datasets are typically **immutable** after being loaded.

This read-only nature enhances performance for analytical queries by simplifying indexing and storage structures.

**Why Are Data Warehouses Necessary?**

**Separation of Workloads**:

OLTP systems are not designed for heavy analytics, and running such queries can degrade their performance.

Data warehouses offload analytics to a dedicated system, ensuring **OLTP performance remains unaffected**.

**Optimization for Analytics**:

Warehouses use **columnar storage** (**storing data column-by-column** rather than row-by-row), which accelerates aggregate operations like sum, average, and filtering across large datasets.

**Indexing and partitioning** allow faster querying by organizing data for efficient retrieval.

**Unified View Across Systems**:

By aggregating data from multiple OLTP systems and external sources, data warehouses provide a **centralized platform** for analytics.

**Examples of Data Warehouses**

**Amazon Redshift**:

A fully managed, petabyte-scale data warehouse on AWS.

Supports massive parallel query execution for high-speed analytics.

**Apache Hive**:

An open-source data warehouse system built on top of Hadoop.

Enables querying and managing large datasets stored in distributed file systems using SQL-like syntax.

**Google BigQuery**:

A serverless, highly scalable data warehouse designed for real-time analytics and machine learning.

**In Practice: Why Not Use OLTP Directly?**

- Running analytics on OLTP systems is like using a compact car to haul heavy cargo—it’s possible, but inefficient and potentially harmful.
- Data warehouses, on the other hand, are like freight trucks: they’re specialized for handling heavy analytical loads, albeit at the cost of real-time interactivity.

**Data Indexes in Relational Databases**

**Motivation**

Before we get into what an index is, let's start with an example to understand why this concept is even needed.

Let's assume we have two tables in a relational database: Company and Employees. Here's what they look like:

The company table stores information about the company. For example:

| ID | Company Name | Company City | Company Country |
| --- | --- | --- | --- |
| 1 | Foo, Inc | San Francisco | US |
| 2 | Bar, Inc | New York | US |
| 3 | Foo Baz, Inc | London | UK |

On the other hand, the Employees table contains information about all the employees of these given companies. For example:

| ID | Company ID | First Name | Last Name |
| --- | --- | --- | --- |
| 1 | 1 | Emily | Mitchell |
| 2 | 1 | Jordan | Turner |
| 3 | 2 | Ethan | Foster |
| 4 | 3 | Olivia | Reynolds |

**Note that Company <-> Employee is a one-to-many relationship, which means that one company can have multiple employees, but an employee can be associated with only one company.** Usually, the way to model this in the database would be to have the company ID as a column of the employees table.

Let's assume that the company table stores hundreds of thousands of records and every company on average has 10 employees. This would mean that the Employees table will have millions of records.

With this in mind, say your system wants to query the list of all Employees belonging to company ID 2. A very simple way of doing that is to scan every row of the Employees table, match the company ID column with the value we want (2 in this case) and then return all the rows where we get a successful match. This will work fine, but the downside is that for every such query, we will have to scan the *entire* Employees table which means that just to get a few rows every single time, we will end up scanning millions of records every single time. This is a *huge performance bottleneck* for systems with a high query rate and large tables. This is where the concept of Indexes comes in.

Now imagine you had some sort of data structure to help you speed up your query. For simplicity, imagine you had a hash map keyed by company ID, and whose values would be a list of all the employee IDs belonging to that company. If you had this data structure, retrieving all employees of a company would be much faster, more efficient and wouldn't require you to scan all the rows of the table. In practice, this is what an **index** is.

To further explain this concept with another example, say you have a 500-page book on databases, and you want to look up the concept of joins. The fastest way to do that would be to look up the index of the book and search for the chapter on joins and directly go to that page, rather than manually scanning all the 500 pages of the book.

**Definition of an Index**

So now that we have a brief idea on why an Index is needed and what it does, let's try to make a slightly more formal definition of an index. We can say that **an** ****index is a data structure that provides a quick and efficient way to look up records in a database table based on the values in one or more columns***.* It works much like an index in a book, allowing you to quickly find the desired information without scanning the entire contents.

https://www.youtube.com/watch?v=NZgfYbAmge8

An index can be on one column or together on a set of columns. When a column is indexed, the database maintains a data structure in a way that reduces the need for full table scans. By doing so, the database can swiftly locate specific rows when a query filters by that indexed column. This efficiency results in quicker query responses and improved overall performance.

**Trade-offs**

**If indexes significantly speed up read queries, why not index all the columns?**

The short answer is that there is no free lunch.

- **Slower writes:** It is true that indexes can significantly improve reads. However, if you account for all DB operations, writes become slower because the index needs to be updated after every write. If there are many insert, update, or delete operations, or if the index itself is already large, adding indexes can introduce a bottleneck in the write operations.
- **Increased memory:** The index data structure itself needs to consume memory. The more indexes you build and the larger your data is, the more memory you would need to store these indexes.

**As a follow-up to the previous question, what are some common indexing mistakes people make in practice?**

The answer is simple: not getting the right balance on what to index on. Many times, people either land up over-indexing or under-indexing.

- **Over-indexing:** Indexing too many columns, even ones that are not going to be queried. This will have a high impact on write operations as well as potentially waste a lot of storage space.
- **Under-indexing:** Indexing too little, in particular missing out on query patterns that often occur, and can stand to gain a lot of performance from an Index. This creates a poor read performance and missed optimization opportunities.

**Okay, so then how does one get the right balance of what to index?**

Getting the right balance and figuring out the right indexes is not a one time simple task. As time passes, the database changes not only in size, but in the kind of data it stores and the query patterns it services. If we want to maintain the right balance, the indexes need to evolve with the changing database as well.

- **Analyze query patterns**: Before selecting an index, analyze the types of queries frequently executed in your database. Understand the SELECT, UPDATE, DELETE, and INSERT operations that are prevalent in your application. For queries involving specific filters or sorting, consider indexes on the columns used in the WHERE clause or ORDER BY clause. This can significantly improve the performance of these operations.
- **Tracking cardinality**: Cardinality refers to the **uniqueness of values in a column**. High cardinality means that the column has mostly unique values, while low cardinality indicates a smaller set of unique values. Columns with **high cardinality,** such as primary keys or columns with mostly unique values, are good candidates for indexing. Indexing such columns enhances search performance. However, when indexing columns with **low cardinality**, it may result in inefficient use of index space. For instance, indexing a Boolean column might not be beneficial due to its limited unique values.
- **Regular monitoring and tuning**: Regularly profile and analyze the performance of your database using monitoring tools. Identify slow queries, high-traffic tables, and areas for improvement. On the other hand, also keep track of index usage statistics to understand which indexes are actively contributing to query performance. You can throw out the ones not needed.

**How Indexing Works**

So now that we know what an index is, let's dig deeper to understand how they actually work, and what data structures are used to power them.

Let's take the example of the employee table again that we saw in the previous chapter.

The query we are trying to speed up is “Give me all employees of company <x>”. Therefore, we’ve decided to index the company ID column in the above table. Let's see how the index is stored and operated internally.

**B-Tree Index**

**Structure**

B-trees are self-balancing tree structures that maintain sorted data. They have a root node, internal nodes, and leaf nodes, with each node containing a certain number of keys and pointers.

In the above Employee table example, with a b-tree index on company ID, we may have a structure that looks something like:

As you can see, the entire range of company IDs are split at every level. The internal nodes store the ranges of the column thats indexed (company ID in this case) and the leaf nodes point to the IDs of the rows in the table (employee IDs in this case). The tree is both a) sorted and b) balanced.

Now when you want to lookup employees of company ID 1, the flow would be:

1. Go to root
2. Traverse to the left internal node (company IDs 1-2)
3. Traverse to the left internal node again (company ID 1)
4. Get all leaves of this node, which are employee IDs 1 and 2

The leaf nodes have the actual IDs of the employees. Since this tree is sorted and balanced, you can get to the leaves (data) of the indexed column in **logarithmic time.**

- This is because at every level, you are eliminating half the remaining data that you have to go through.
- Another way of looking at it is that the tree is binary and balanced which means the depth of the tree is in the order of log(n), where n is the number of distinct values of the indexed column.

**Use Cases**

B-trees are **suitable for range queries and ordered data retrieval**. Commonly used in scenarios where data needs to be sequentially accessed, such as in-range searches. This is because data is stored in a sorted fashion.

**Range Queries**: Optimized for retrieving ordered data (e.g., "Company ID between 1 and 10").

**Sequential Access**: Efficient for operations requiring sorted data.

**Quick miscellaneous notes**

- Recall that indexes in general provide fast reads but at the cost of increased write latencies. In the case of B-tree indexes, since they are balanced and sorted, writes may slow down not only from adding new nodes but because the **tree itself may need to re-balance**.
- If the database is small and query patterns seem like point lookups, it just makes sense to use a **hash index in those cases** (see below).
- This kind of structure where the index entries are organized in a tree-like structure with multiple levels is called **multi-level index**.

**Structure**

A B-tree is a self-balancing, sorted tree structure designed for efficient data retrieval. It consists of:

- **Root Node**: The top-most node.
- **Internal Nodes**: Store ranges of the indexed column (e.g., company IDs) and pointers to child nodes.
- **Leaf Nodes**: Contain pointers to the actual table data (e.g., employee IDs).

In a B-tree indexed table (e.g., Employee table with a company ID index), the tree is both **sorted** and **balanced**, enabling logarithmic time complexity for lookups.

**Example Lookup (Company ID = 1)**:

1. Start at the root node.
2. Navigate to the appropriate internal node (range: 1–2).
3. Traverse further to the specific node (Company ID = 1).
4. Retrieve employee IDs from the leaf nodes.

**Performance**

The tree’s balance ensures that each lookup requires at most `log(n)` operations, where `n` is the number of distinct values in the indexed column.

**Write Latencies**: Writes are slower due to potential rebalancing when inserting or deleting nodes.

**Alternative Indexes**: For small datasets or frequent point lookups, **hash indexes** may be faster, as they avoid the overhead of maintaining a tree structure.

**Multi-Level Index**: B-tree indexes use multiple levels to organize entries, which reduces the number of disk accesses needed for retrieval.

**Hash Index**

**Structure**

Hash indexes use a hash function to map keys to specific locations in a fixed-size array of buckets. Each bucket contains a linked list of entries with the same hash value.

In our example, assume we have an array of buckets that can be easily looked up.

Assume we have a hash function which hashes company IDs as

- f(1) = Bucket 1
- f(2) = Bucket 2
- f(3) = Bucket 3

The concept of hashing here is the same as the HashMap data structure. You have a hash function that, in constant time, can take an input and map it to a particular location in memory. Here, since the index is on company ID, the hash function will take a company ID as the input and map it to a location where the corresponding employee IDs of that company are stored. This will happen in amortized O(1) time.

In each bucket are the employee ids of those companies whose hash points to that bucket. When a request comes in, you apply the hash function which takes you straight to the bucket and the information in the bucket is what you are looking for.

**Use Cases**

Hash indexes are effective for equality searches, where the goal is to quickly locate a specific record based on a key. Well-suited for scenarios with in-memory databases and primary key indexing.

**Equality Searches**: Best for queries like “Company ID = 1”.

**In-Memory Databases**: Ideal for scenarios where quick point lookups are critical, such as **primary key indexing**.

**Quick Miscellaneous Notes**

- Hash indexes are not optimized for sorting and range queries. B-tree indexes are much more suitable for those kinds of queries. The reason is that B-tree stores the indexed column in a sorted way already. For hash indexes, not only is there no way to store indexed columns in a sorted way, but typically, data within each hashed bucket is also unsorted.
- Hash indexes may perform poorly in scenarios where the hash function poorly distributes keys across buckets. If there are a lot of keys in the same bucket, this will lead to collisions and therefore a degradation of the query performance because within the bucket, you will still have to linearly search for the data. (This is a property of hash map data structures in general.)
- This is a **single level index**. All the index entries are stored in a single, flat structure, allowing for direct access to the data.

**Not Suitable for Range Queries**: Hash indexes are not optimized for range or ordered queries, as they do not maintain data in a sorted manner (unlike B-trees).

**Collisions**: Poor hash distribution can result in many keys mapping to the same bucket, requiring linear search within the bucket and degrading performance.

**Single-Level Index**: Hash indexes have a flat structure, with no hierarchical levels, ensuring direct access to data.

**Composite Indexes**

Indexes need not necessarily be just on one column. Consider this table, which denotes different types of relationships between two users:

Now, say we want to query all Friends of User 10. If we simply had an index on the User ID 1 column, then matching that with ID 10 would *give us all relationships* of User ID 10. Then we would have to do another pass and filter the “FRIEND” relation rows and return those as the result.

This is not the best use of the index, specially if each user has tons of different relationships with different users. Instead, what we can do is have a ***multi-column index*** on (USER ID 1, Relation). This basically means that the B-tree index is built on the user and relation together — think concatenating user ID and relation — and having that as the sort key.

Now we look up the index for (10, Friend) which easily gets us to the friends of User 10.

A **composite index** is an index built on multiple columns, useful for queries that filter on a combination of fields.

**Example:**
For a table recording relationships between users (`User ID 1`, `Relation`, `User ID 2`), querying "Friends of User 10" benefits from a composite index on `(User ID 1, Relation)`:

Without composite index: A single-column index on `User ID 1` retrieves all relationships for User 10, requiring additional filtering for “Friend.”

With composite index: The `(User ID 1, Relation)` index efficiently narrows down to `(10, Friend)` directly.

**Quick notes**

- While creating the composite index, the order of columns ***does*** matter. If a composite index is created on columns (A, B, C), the index is efficient for queries that filter on A and B or just on A. However, it might be less effective for queries filtering only on columns B or C.
- The size of the composite index is influenced by the order of columns. The more columns included in the index, the larger the index size.
    
    **Column Order**: The sequence of columns matters. For `(A, B, C)`, the index is effective for:
    
    - Queries filtering on `A`.
    - Queries filtering on `A, B`.
    - Less effective for filtering on just `B` or `C`.
    
    **Size**: Adding columns increases the index size. Balance index utility with storage and write performance costs.
    

**Structure**

A **hash index** maps keys to specific locations in an array of buckets using a hash function. Each bucket contains entries (e.g., employee IDs) associated with keys that hash to the same value.

Example:

Using a hash function on `Company ID`:

- `f(1)` → Bucket 1
- `f(2)` → Bucket 2
- `f(3)` → Bucket 3

The hash function directly maps a `Company ID` to a bucket, where the corresponding data is stored. This enables **amortized O(1)** time complexity for lookups.

**Non-Relational Database Indexes and Inverted Indexes**

**Can we index fields in non-relational databases too? Or are indexes only for relational databases?**

Let's assume we have a non-relational database which stores metadata about a user. The structure looks like:

```
{
  "User_ID_1": {
    "First_name": "Foo",
    "Last_name": "Bar",
    "City": "San Francisco",
    "Country": "USA",
    "Profession": "Student",
    ...
  },
  ...
}

```

Now, if we wanted to query all the users living in London, a naive way of doing it would be to scan all the rows of the database and return the ones where the City value matches. Of course, in practice, this would lead to a huge performance bottleneck and wouldn’t allow the system to scale.

Again, this is where indexes come into the picture. Indexes are also a feature of non-relational databases. We can build a hash map or another data structure that can look up rows based on an attribute value without a full database scan.

In this example, having an index on the City attribute would mean that any query that would have a City clause or a city filter would not need a full database scan. Instead, the Index would have a mapping for every City to its corresponding rows in the database, thus making the read queries really fast.

Indexes are used in non-relational databases to improve query performance by avoiding full scans. For example, indexing the `City` field allows quick lookup of users in a specific city without scanning all rows.

Just like relational databases, non-relational database indexes can be:

1. **Single-field Indexes**: Where the index is only on one field, just like we saw above.
    1. **Single-field Indexes**: Indexes on one field, like `City`.
2. **Compound indexes**: Where you can index on a set of fields. Just like we saw with composite indexes in relational databases, the order of the fields in the compound index matters. Data is grouped by the first field in the index and then by each subsequent field.
    1. **Compound Indexes**: Indexes on multiple fields, such as `City` + `Country`, where the order matters for grouping data.

**Inverted Indexes**

Now, let’s understand how the use case of text searching might not be fully served with the indexes we’ve seen so far.

Inverted indexes are designed for efficient text searching when traditional indexes fail to handle substring queries effectively.

**Key Idea:**

An inverted index breaks text (e.g., sentences) into individual terms (words) and maps each term to the rows (or documents) where it appears.

Suppose we have a system that stores a small “about me” section of a user:

| ID | About |
| --- | --- |
| 1 | Alice is a Software engineer from London |
| 2 | Bob is a Doctor from Chicago |
| 3 | Charlie is a data scientist from Seattle |
| 4 | Dylan is an accountant from London |

Now, let’s suppose we wanted to query all the rows where the word London appeared. A traditional single field index we’ve seen so far will ***not work.*** This is because for each row of the table, the entire string as a single unit is treated as the one value, the key of the index. Therefore,

- For b-tree indexes, the entire sentence would be taken as a string and stored in the index in a sorted way.
- For hash indexes, the entire sentence would be the “key” which when hashed would lead to the correct rows.

In both cases, a query for an ***exact match*** of the *entire* string (”Alice is a Software engineer from London” in this case) would benefit from the index but not any arbitrary substring (like just the word “London”).

With this structure, querying for “London” would give you all the Row IDs where that value occurs.

A common real-life example of an inverted index is the index found at the end of a book. This index lists important words alphabetically and provides the page numbers where each word appears. Instead of reading the entire book to find a specific word, you can quickly locate the word and its associated page numbers in the index, making your search much more efficient.

In practice, inverted indexes process text through many more phases than just splitting a sentence into words. Here are some of the phases:

**Tokenization**: Splitting text into individual words or terms.

**Normalization**: Converting terms to a standard format (e.g., lowercase).

**Stemming**: Reducing words to their root form (e.g., "running" to "run").

**Stop Word Removal**: Eliminating common words (e.g., "and", "the") that add little value.

Some good examples of systems that provide text searching using inverted indexes are ElasticSearch and Apache Solr.

**Scaling Database in Distributed Systems**

The Need for Distributed Systems

In today’s world where systems serve tens or even hundreds of millions of users, and deal with petabytes of data, the use of distributed systems has become inevitable. It’s almost impossible to have systems of such scale not having multiple machines involved in the storage and retrieval of data.

Here are a few of the reasons we might want to distribute our database across multiple machines:

 **Scalability**

Our read or write traffic load may grow to be more than a single machine can handle.

**Fault Tolerance**

Machines go down all the time. If we want to make sure that our systems are still up and running and not falling victim to a single machine failing, we might want to have multiple copies of the database distributed across multiple machines. This will give us better availability and fault tolerance.

**Latency**

If people from all over the globe are using our systems, having servers located in just one location can compromise the performance and experience for people who are accessing the system from a far off geographical location. In such cases, having servers distributed in multiple locations across the globe can help maintain a high quality of service.

**Vertical and Horizontal Scaling**

Thus far, we learned about database fundamentals, query patterns and important concepts. Now, we will see the same concepts but in a distributed environment. We will see how the fundamentals we covered previously are executed in a distributed environment, where multiple machines are involved, and the challenges get tougher. Before we jump into some of the more detailed topics like sharding and replication, let’s quickly recap some significant concepts related to scaling. Recall the two types of scaling:

**Vertical Scaling**

Vertical scaling involves adding more resources (CPU, RAM, disk space) to an existing machine to handle increased load. This involves enhancing the capacity of the current machine by adding more powerful processors, more memory, or faster storage. This is also known as scaling up.

The improvements are made to a single node rather than adding more nodes to the system. This is the first choice of scaling for traditional relational databases like MySQL, PostgreSQL, and Oracle.

Vertical scaling is simpler to implement and manage, as it involves a single or few nodes. It also needs no modification to the application layer for handling distributed data. Finally, it is easier to maintain strong consistency and ACID properties because data still all lives together.

On the other hand, usually there is an upper limit to resources that can be added to a single machine. Vertical scaling can help is scaling the system up to a certain limit, but after that horizontal scaling becomes a better choice. This is because with vertical scaling, we still have a single point of failure and adding more resources to a single node can become expensive when compared to adding more nodes.

**Horizontal Scaling**

Horizontal scaling involves adding more nodes to a distributed database system to handle increased load and storage requirements. Data is partitioned or sharded across different nodes. Each node holds a portion of the total data set. This is also known as scaling out.

Horizontal scaling provides better fault tolerance and is more suitable for handling very large datasets and high throughput. This is a better choice of scaling after a point where vertical scaling becomes too expensive or unable to take the load any further. Non-relational databases do well with horizontal scaling, since data is usually key-value pairs that can easily be distributed across different machines without having to worry about joins and transactional support.

However, distributing data across machines comes with some disadvantages. One of the biggest ones is the native ACID support that relational databases provide. With data distributed across nodes, providing transactional support becomes hard to achieve. Imagine having to lock rows across different machines and all the communication that needs to happen between the machines to make sure transactional support is provided. Even querying joins becomes tough because queries now have to join across different tables living on entirely different machines. This is why relational databases prefer vertical scaling over horizontal scaling.

**Partitioning a.k.a Sharding**

At one point when the data gets too large to fit onto a single machine, or the traffic gets too high for one machine to handle, it might become necessary to split the database across multiple machines. This process of splitting the data across multiple machines is known as **partitioning** or **sharding**, and each machine with data is referred to as a **shard**.

Let's look at some ways we can partition our data across various machines.

**Key Range Partitioning**

One sharding strategy is to take your entire key range, split it and divide it across multiple shards.

For example, let's assume we have a database of people names keyed by first name.

Say we want to split this database across 2 shards. One example of key range partitioning would be putting names starting with A to M would on shard 1 and names starting with N to Z on shard 2.

This means every query that query or update first names starting from A to M would hit shard 1, and the rest would hit shard 2.

**Pros**

This strategy works well because it's relatively easy to implement and the main server taking in requests and routing it to the right shard would have a relatively simple logic of routing requests. The request router needs to have the mapping table of the boundaries of every shard so that it can route the request to the right shard.

Resharding is fairly straightforward. Say there are a lot more names from A-M and that shard fills up quickly. When this happens, we only need to migrate a small subset of the entire data to the new shard.

Another advantage we get with key range sharding is that the data is still sorted by keys, which means that range queries become very efficient to do. For example, if our key was created date, then all the rows created on the same day would land up on the same shard. Therefore, queries like “provide all rows created yesterday” would only have to look up one shard (assuming all rows created on one day fit on one shard), rather than looking up all the shards.

**Cons**

However, this strategy has some disadvantages too. Based on what the data looks like, it's possible that some shards gets an unfairly high amount of traffic as compared to other shards. For example, if there are many more names starting with A than with Z, one particular shard will land up serving a disproportionately high amount of traffic. Similarly, with the example where created date is the key, every single insert on one day will have to be handled by one shard. This way we land up with ***“Hot Shards”*** and unbalanced traffic.

**Hash Partitioning**

The aim of sharding should be to evenly distribute traffic across all partitions. We saw how key range sharding might perform poorly with that. Another partitioning strategy that could help with this problem is partitioning by hash. In this strategy, instead of using the primary key, we pass the primary key through a hash function that returns a number. Then, we assign shards to serve a range of numbers.

To continue with our example with names, let's assume we have a hash function that maps the following way:

We see that the hash function returns an integer between 0 and 10, and now we can have 2 shards, shard 1 servicing rows with hashes between 0 and 5 and the other between 6 and 10.

**Pros**

With this strategy, we can more easily distribute traffic evenly. Even keys close together might have totally different hash values and land up on different shards, thereby balancing traffic a lot more fairly.

For resharding, similar to term sharding, a new shard can pick up a subset of the hash range of the current shard, thereby only having to move those particular rows and not the entire table.

The request router logic is also fairly simple, it has the mapping of every range to the shard that serves that range.

**Cons**

However, with this strategy, we lose storing keys in a sorted fashion. This means that queries might have to hit a lot more shards (in some cases, every get request would hit all shards) in order to get the result. In the example where the key was the created time, even rows created in a small span of time can land up on totally different shards. Therefore, a query like “give me all rows created yesterday” would hit all shards. This query pattern, where we need to query across all shards, is also known as ***scatter-gather.***